{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as tc\n",
    "from torch.utils.data import DataLoader\n",
    "from data_set_.data_loader_ import ISITDataset_gen,make_df_from_data_dir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from contextlib import nullcontext\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F  \n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accu(x,y,title,\n",
    "    suptitile= \"Unimodal Classification\",\n",
    "    xlabel='number of events to detection',\n",
    "    ylabel='probability of correct classification',\n",
    "    ):\n",
    "  fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "  fig.suptitle(suptitile, fontsize=16)\n",
    "  ax.set_title(title,fontsize=12)\n",
    "  ax.set_ylabel(ylabel,fontsize =12)\n",
    "  ax.set_xlabel(xlabel,fontsize =12)\n",
    "  ax.set_ylim([0, 1])\n",
    "  plt.yticks(np.arange(0, 1+0.05, 0.05))\n",
    "  plt.grid(True)\n",
    "  ax.scatter(x,y)\n",
    "  plt.tight_layout() \n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretizer for gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer:\n",
    "  def __init__(self,cont_max,token_num):\n",
    "    self.cont_max = cont_max\n",
    "    self.token_num = token_num\n",
    "    self.scale = (token_num-1)/cont_max  \n",
    "    self.interval  = cont_max/(token_num-1)\n",
    "  def cont_2_token(self,cont_values):\n",
    "    assert cont_values.max()<=self.cont_max\n",
    "    token_values_raw=cont_values*self.scale  \n",
    "    token_values = token_values_raw.to(dtype=tc.int64)\n",
    "    assert token_values.max()<=self.token_num\n",
    "    return token_values\n",
    "  def token_2_cont(self,token_values):\n",
    "    token_cont = token_values.to(tc.float32)\n",
    "    cont_values_raw = token_cont/self.scale\n",
    "    cont_values = cont_values_raw +\\\n",
    "         tc.rand(token_values.shape)*self.interval\n",
    "    assert cont_values.max()<=self.cont_max\n",
    "    return cont_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT configuration\n",
    "get the vocabulary size and setup gpt configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/isit-2024-bits-and-bots/ISIT_Dataset/hlisa_traces\n",
      "./data/isit-2024-bits-and-bots/ISIT_Dataset/gremlins\n",
      "./data/isit-2024-bits-and-bots/ISIT_Dataset/za_proxy\n",
      "./data/isit-2024-bits-and-bots/ISIT_Dataset/survey_desktop\n",
      "./data/isit-2024-bits-and-bots/ISIT_Dataset/random_mouse_with_sleep_bot\n",
      "hlisa_traces\n",
      "gremlins\n",
      "za_proxy\n",
      "survey_desktop\n",
      "random_mouse_with_sleep_bot\n",
      "hlisa_traces Index(['userId', 'x', 'y', 'eventName', 'userType', 'time_diff'], dtype='object') 398788\n",
      "unique user 1166 / 398788\n",
      "gremlins Index(['userId', 'x', 'y', 'eventName', 'userType', 'time_diff'], dtype='object') 185657\n",
      "unique user 577 / 185657\n",
      "za_proxy Index(['userId', 'x', 'y', 'eventName', 'userType', 'time_diff'], dtype='object') 46426\n",
      "unique user 853 / 46426\n",
      "survey_desktop Index(['userId', 'x', 'y', 'eventName', 'userType', 'time_diff'], dtype='object') 650641\n",
      "unique user 64743 / 650641\n",
      "random_mouse_with_sleep_bot Index(['userId', 'x', 'y', 'eventName', 'userType', 'time_diff'], dtype='object') 183365\n",
      "unique user 532 / 183365\n",
      "0 0\n",
      "x 1195 hlisa_traces\n",
      "y 2798 hlisa_traces\n",
      "x 1061 gremlins\n",
      "y 3190 gremlins\n",
      "x 862 za_proxy\n",
      "y 3119 za_proxy\n",
      "x 2840 survey_desktop\n",
      "y 4427 survey_desktop\n",
      "x 1799 random_mouse_with_sleep_bot\n",
      "y 899 random_mouse_with_sleep_bot\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "  n_layer:int = 6;  n_head:int = 6;  n_embd:int = 384\n",
    "  block_size:int = 256 \n",
    "  bias:bool = False \n",
    "  vocab_size:int = None   \n",
    "  dropout:float = 0.2 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "\n",
    "device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\n",
    "gpt_conf = GPTConfig()\n",
    "train_data_ratio = 0.8\n",
    "df_dict_all,df_dict_train,df_dict_eval = make_df_from_data_dir(train_data_ratio = train_data_ratio)\n",
    "max_yyy = 0;max_xxx =0;max_ttt=0\n",
    "print(max_xxx,max_yyy)\n",
    "for k in df_dict_all.keys():\n",
    "  x_max = df_dict_all[k]['x'].to_numpy().max()\n",
    "  y_max = df_dict_all[k]['y'].to_numpy().max()\n",
    "  t_max = df_dict_all[k]['time_diff'].to_numpy().max()\n",
    "  print('x',x_max,k)\n",
    "  print('y',y_max,k)\n",
    "  if x_max > max_xxx: max_xxx = x_max\n",
    "  if y_max > max_yyy: max_yyy = y_max\n",
    "  if t_max > max_ttt: max_ttt = t_max\n",
    "vocab_size = max(max_xxx,max_yyy)+1  \n",
    "gpt_conf.vocab_size = vocab_size\n",
    "# np.sort(df_dict_all['survey_desktop']['time_diff'].to_numpy()\n",
    "disc = Discretizer(max_ttt,vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant file ckpt path, train from scratch\n"
     ]
    }
   ],
   "source": [
    "# Use Training set for ISIT2024\n",
    "max_sample_len_train = 70\n",
    "min_sample_len_train = 3\n",
    "training_len  =600#* 1000  \n",
    "eval_len = 10\n",
    "bs = 128\n",
    "load_from_checkpoint = True\n",
    "\n",
    "ckpt_path = 'gpt.ckpt'\n",
    "if not os.path.isfile(ckpt_path):\n",
    "  print(\"cant file ckpt path, train from scratch\")\n",
    "  load_from_checkpoint = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ISITDataset_gen(df_dict_train,training_len=training_len*bs,\n",
    "                            min_sample_len=min_sample_len_train,\n",
    "                            max_sample_len=max_sample_len_train)   \n",
    "eval_dataset  = ISITDataset_gen(df_dict_eval,training_len=eval_len*bs,\n",
    "                            min_sample_len=max_sample_len_train,\n",
    "                            max_sample_len=max_sample_len_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model GPT  \n",
    "some utils function for gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "  if isinstance(module, nn.Linear):\n",
    "    tc.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    if module.bias is not None:\n",
    "        tc.nn.init.zeros_(module.bias)\n",
    "  elif isinstance(module, nn.Embedding):\n",
    "    tc.nn.init.normal_(module.weight, mean=0.0, std=0.02)  \n",
    "def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "  param_dict = {pn: p for pn, p in self.named_parameters()}# start with all candidate params\n",
    "  param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}# filter out those that do not require grad\n",
    "  #create optim groups. Any params that is 2D will be weight decayed, otherwise no.\n",
    "  # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "  decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "  nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "  optim_groups = [\n",
    "     {'params': decay_params, 'weight_decay': weight_decay},\n",
    "     {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "  ]\n",
    "  num_decay_params = sum(p.numel() for p in decay_params)\n",
    "  num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "  print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "  print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "  # Create AdamW optimizer and use the fused version if it is available\n",
    "  fused_available = 'fused' in inspect.signature(tc.optim.AdamW).parameters\n",
    "  use_fused = fused_available and device_type == 'cuda'\n",
    "  extra_args = dict(fused=True) if use_fused else dict()\n",
    "  optimizer = tc.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "  print(f\"using fused AdamW: {use_fused}\")\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "  \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "  def __init__(self, ndim, bias):\n",
    "    super().__init__()\n",
    "    self.weight = nn.Parameter(tc.ones(ndim))\n",
    "    self.bias = nn.Parameter(tc.zeros(ndim)) if bias else None\n",
    "  def forward(self, input):\n",
    "    return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "  def __init__(self, c:GPTConfig):\n",
    "    super().__init__()\n",
    "    assert c.n_embd % c.n_head == 0\n",
    "    self.c_attn = nn.Linear(c.n_embd, 3*c.n_embd,bias=c.bias)# qkv projections for all heads,in a batch\n",
    "    self.c_proj = nn.Linear(c.n_embd, c.n_embd, bias=c.bias)# output projection\n",
    "    # regularization\n",
    "    self.attn_dropout=nn.Dropout(c.dropout);self.resid_dropout=nn.Dropout(c.dropout)\n",
    "    self.dropout = c.dropout\n",
    "    self.n_head = c.n_head;  self.n_embd = c.n_embd\n",
    "    # causal mask to ensure that attn is only applied to left in input seq\n",
    "    self.register_buffer(\"bias\", tc.tril(tc.ones(c.block_size, c.block_size))\n",
    "                                .view(1, 1, c.block_size, c.block_size))\n",
    "  def forward(self, x,visible_end):\n",
    "    B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "    q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "    att_raw = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "    \n",
    "    att_raw[tc.arange(B),:,:,visible_end[0]] = att[tc.arange(B),:,:,visible_end[0]]\n",
    "    att_raw[tc.arange(B),:,:,visible_end[1]] = att[tc.arange(B),:,:,visible_end[1]]\n",
    "    att_raw[tc.arange(B),:,:,visible_end[2]] = att[tc.arange(B),:,:,visible_end[2]]\n",
    "    att = att_raw    #att (128,n_head=6,209,209)   vis_end.max(208)\n",
    "    #todo\n",
    "    # breakpoint()\n",
    "    att = F.softmax(att, dim=-1)\n",
    "    att = self.attn_dropout(att)\n",
    "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    # breakpoint()\n",
    "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "    # output projection\n",
    "    y = self.resid_dropout(self.c_proj(y))\n",
    "    return y\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, c:GPTConfig):\n",
    "    #c(n_layer=6,n_head=6,n_embd=384,block_size=256,bias=F,vocab_size=65,dropout=0.2)\n",
    "    super().__init__()\n",
    "    self.c_fc    = nn.Linear(c.n_embd, 4*c.n_embd, bias=c.bias)\n",
    "    self.gelu    = nn.GELU()\n",
    "    self.c_proj  = nn.Linear(4 * c.n_embd, c.n_embd, bias=c.bias)\n",
    "    self.dropout = nn.Dropout(c.dropout)\n",
    "  def forward(self, x):\n",
    "    x = self.c_fc(x)\n",
    "    x = self.gelu(x)\n",
    "    x = self.c_proj(x)\n",
    "    x = self.dropout(x)\n",
    "    return x\n",
    "class Block(nn.Module):\n",
    "  def __init__(self, c:GPTConfig):\n",
    "    super().__init__()\n",
    "    self.ln_1 = LayerNorm(c.n_embd, bias=c.bias)\n",
    "    self.attn = CausalSelfAttention(c)\n",
    "    self.ln_2 = LayerNorm(c.n_embd, bias=c.bias)\n",
    "    self.mlp = MLP(c)\n",
    "  def forward(self, x,visible_end):\n",
    "    x = x + self.attn(self.ln_1(x),visible_end)\n",
    "    x = x + self.mlp(self.ln_2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "  def __init__(self, c:GPTConfig):\n",
    "    super().__init__()\n",
    "    assert c.vocab_size is not None; assert c.block_size is not None\n",
    "    self.c = c\n",
    "    self.wte = nn.Embedding(c.vocab_size, c.n_embd)\n",
    "    self.lm_head = nn.Linear(c.n_embd, c.vocab_size, bias=False)\n",
    "    self.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "    self.wpe = nn.Embedding(c.block_size, c.n_embd)\n",
    "    self.drop = nn.Dropout(c.dropout)\n",
    "    self.h = nn.ModuleList([Block(c) for _ in range(c.n_layer)])\n",
    "    self.ln_f = LayerNorm(c.n_embd, bias=c.bias)\n",
    "    \n",
    "    self.apply(init_weights)# init all weights\n",
    "    for pn, p in self.named_parameters():# apply special scaled init to the residual projections, per GPT-2 paper\n",
    "      if pn.endswith('c_proj.weight'):\n",
    "        tc.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * c.n_layer))\n",
    "  def forward(self, idx, targets=None,terminate_index=None):\n",
    "    device = idx.device\n",
    "    b, t = idx.size() #*idx [bs,sentence_len=block_len]#*ex if prompt = \"hello\",then block len = 5\n",
    "    assert t <= self.c.block_size, f\"cant forward seq of len {t}, block size is only {self.c.block_size}\"\n",
    "    \n",
    "    if terminate_index != None:\n",
    "      term_idx = (terminate_index*3)\n",
    "      term_idx = term_idx.reshape(1,-1)\n",
    "      visible_end = tc.cat((term_idx-3,term_idx-2,term_idx-1))    \n",
    "      max_end = visible_end.max()\n",
    "      assert max_end <= idx.shape[1]\n",
    "      if max_end == idx.shape[1]:\n",
    "        visible_end = tc.where(visible_end==max_end,max_end-1,visible_end)\n",
    "    pos = tc.arange(0, t, dtype=tc.long, device=device) # shape (t)  #*pos =[0]\n",
    "    # pos = [1,2,3,...,sentence_len] for pos embed\n",
    "    tok_emb = self.wte(idx) # token embed (b,t)->(b, t, n_embd)\n",
    "    pos_emb = self.wpe(pos) # pos embed     (t)->   (t, n_embd)\n",
    "    x = self.drop(tok_emb + pos_emb) # x(b,t,n_embd)\n",
    "    for block in self.h:\n",
    "      x = block(x,visible_end)\n",
    "    x = self.ln_f(x)  \n",
    "    logits = self.lm_head(x)#(b,t,n_embd=384)->(b,t,vocab_size=65)\n",
    "    if targets is not None:  # if we are given some desired targets also calculate the loss\n",
    "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-1)\n",
    "    else:\n",
    "      loss = None\n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 12,415,488 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: False\n",
      "1/600, term_idx 17, loss.item()=7.9279937744140625\n",
      "2/600, term_idx 69, loss.item()=5.237119674682617\n",
      "tensor([15.])\n",
      "3/600, term_idx 48, loss.item()=4.071180820465088\n",
      "4/600, term_idx 45, loss.item()=3.7404303550720215\n",
      "tensor([1.])\n",
      "5/600, term_idx 57, loss.item()=3.6051461696624756\n",
      "6/600, term_idx 29, loss.item()=3.5577993392944336\n",
      "tensor([179.])\n",
      "7/600, term_idx 9, loss.item()=3.4530069828033447\n",
      "tensor([53.])\n",
      "8/600, term_idx 40, loss.item()=3.53372859954834\n",
      "9/600, term_idx 29, loss.item()=3.504119396209717\n",
      "10/600, term_idx 44, loss.item()=3.161428928375244\n",
      "11/600, term_idx 52, loss.item()=3.186582326889038\n",
      "12/600, term_idx 21, loss.item()=3.1978440284729004\n",
      "13/600, term_idx 58, loss.item()=3.4967141151428223\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "model = GPT(gpt_conf).to(device)\n",
    "#** gpt blocksize  \n",
    "blocksize = max_sample_len_train*3-1\n",
    "beta1 = 0.9;  beta2 = 0.99  \n",
    "learning_rate:float = 1e-3;weight_decay:float = 1e-1;grad_clip:float = 1.0\n",
    "# optim = tc.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "optim=configure_optimizers(model,weight_decay, learning_rate, (beta1,beta2), device)\n",
    "if load_from_checkpoint == False:\n",
    "  for batch in train_dataloader:\n",
    "    # for task A. Defense Task,  only the time_diff and position [x,y] elements recorded for each event can be used as input to classifier  \n",
    "    _, time_diff, pos_x, pos_y, _,terminate_idx = batch  \n",
    "    #* pos_x,pos_y(bs=128,n_of_events=70),terminate_idx(bs=128),userType(bs=128)\n",
    "    time_diff_token = disc.cont_2_token(time_diff).to(dtype=tc.float32)  #todo use log for small numbers\n",
    "    pos_xyt = tc.stack((pos_x,pos_y,time_diff_token),dim=-1).reshape(bs,-1)  #(bs=128,n_of_events*2 = 140)\n",
    "    if tc.any(time_diff_token>0):\n",
    "      print(time_diff_token[time_diff_token>0])  #todo  use log at discretizer\n",
    "    assert  (pos_xyt.max()<=vocab_size) and (pos_xyt.min()>=0) and (time_diff.max()<= max_ttt)\n",
    "    pos_xyt = pos_xyt.to(device=device,dtype=tc.int64)  #* (bs=128,len=140)\n",
    "    xx = pos_xyt[:,:blocksize]\n",
    "    yy = pos_xyt[:,1:blocksize+1]\n",
    "    assert xx.shape[0]==bs and xx.shape[1]==blocksize and yy.shape[0]==bs and yy.shape[1]==blocksize\n",
    "    assert xx.max()<vocab_size and yy.max()<vocab_size\n",
    "    _,loss = model(xx,yy,terminate_idx)\n",
    "    optim.zero_grad() \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    idx+=1\n",
    "    # breakpoint()\n",
    "    print(f\"{idx}/{training_len}, term_idx {terminate_idx[0].item()}, {loss.item()=}\")\n",
    "    assert min_sample_len_train<=terminate_idx[0].item()<=max_sample_len_train\n",
    "  tc.save({'state_dict':model.state_dict()},ckpt_path)\n",
    "else:\n",
    "  checkpoint = tc.load(ckpt_path)\n",
    "  model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to generate some samples using gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "num_samples = 3 # number of samples to draw\n",
    "max_new_tokens = blocksize+1 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if tc.cuda.is_available() and tc.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "ptdtype = {'float32':tc.float32,'bfloat16':tc.bfloat16,'float16':tc.float16}[dtype]\n",
    "ctx = nullcontext() if device == 'cpu' else tc.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "# encode the beginning of the prompt\n",
    "x = 21;y = 153;t = 0\n",
    "end_idx = 3*50\n",
    "start_xyt = (tc.tensor([x,y,t], dtype=tc.long, device=device)[None, ...])\n",
    "ini_xyt = tc.zeros((1,max_new_tokens),dtype=tc.long, device=device)\n",
    "ini_xyt[0,0] = 21   #x \n",
    "ini_xyt[0,1] = 153  #y\n",
    "ini_xyt[0,2] = 0    #t\n",
    "ini_xyt[0,end_idx] = 56  #x  \n",
    "ini_xyt[0,end_idx+1] = 321  #y\n",
    "ini_xyt[0,end_idx+2] = 0    #t\n",
    "#*** x.shape  [1,3]=  bs, start_sentence_len\n",
    "# run generation\n",
    "@tc.no_grad()\n",
    "def generate(model_, idx, end_idx_, temperature=1.0, top_k=None):\n",
    "  \"\"\"Take condition seq of indices (b,t):tc.Long \n",
    "  complete seq max_new_tokens times, \n",
    "  feed predictions back into model each time.\"\"\"\n",
    "  assert idx.size(1) <= model_.c.block_size\n",
    "  for ii in range(2,end_idx_-1):\n",
    "    # if seq context grow too long we must crop it at block_size\n",
    "    idx_cond = idx \n",
    "    logits, _ = model_(idx_cond,None,terminate_index=tc.tensor([(end_idx_+3)//3],dtype=tc.int64)) \n",
    "    logits = logits[:,[ii],:]\n",
    "    logits = logits[:, -1, :] / temperature# pluck logits at final step and scale by desired temperature\n",
    "    if top_k is not None:# optionally crop the logits to only the top k options\n",
    "      #* topk=200\n",
    "      v, _ = tc.topk(logits, min(top_k, logits.size(-1)))\n",
    "      logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "    # apply softmax to convert logits to (normalized) probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # sample from the distribution\n",
    "    idx_next = tc.multinomial(probs, num_samples=1)\n",
    "    # append sampled index to the running sequence and continue\n",
    "    # idx = tc.cat((idx, idx_next), dim=1)  #add the newly gen last word to the end of the sentence \n",
    "    idx[0,ii+1]= idx_next[0,0]\n",
    "  return idx\n",
    "with tc.no_grad():\n",
    "  with ctx:\n",
    "    for k in range(num_samples):\n",
    "      gen_xyt = generate(model,ini_xyt,end_idx, temperature=temperature, top_k=top_k)\n",
    "      print(gen_xyt)\n",
    "      bs,bl = gen_xyt.shape \n",
    "      gen_xyt = gen_xyt.reshape(bs,-1,3)  \n",
    "      print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation   \n",
    "evaluate using the unimodal-model trained in 1.a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nets.nn_net import NeuralNetwork\n",
    "from data_set_.data_loader_ import ISITDataset\n",
    "max_sample_len_eval = 70  #*70\n",
    "bs_eval = 1\n",
    "eval_len = 100\n",
    "eval_dataset_mlp  = ISITDataset(df_dict_eval,training_len=eval_len*bs_eval,\n",
    "                            min_sample_len=max_sample_len_eval,\n",
    "                            max_sample_len=max_sample_len_eval)\n",
    "eval_dataset  = ISITDataset_gen(df_dict_eval,training_len=eval_len*bs_eval,\n",
    "                            min_sample_len=max_sample_len_eval,\n",
    "                            max_sample_len=max_sample_len_eval)\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=bs_eval, \n",
    "                             shuffle=True)\n",
    "\n",
    "category = len(eval_dataset_mlp.idx_2_name)  #5 #['hlisa_traces', 'gremlins', 'za_proxy', 'survey_desktop', 'random_mouse_with_sleep_bot']\n",
    "ckpt_path = 'model_a1.ckpt'\n",
    "model_mlp = NeuralNetwork(max_len=max_sample_len_train,\n",
    "                      output_size=category).to(device)\n",
    "if not os.path.isfile(ckpt_path):\n",
    "  raise Exception('you dont have defensive task ckpt, please run \"python main.py\" first') \n",
    "checkpoint = tc.load(ckpt_path)\n",
    "model_mlp.load_state_dict(checkpoint['state_dict'])\n",
    "thres_num = 10\n",
    "conf_thres_list = np.linspace(0,1,thres_num+1)[1:-1]\n",
    "#*  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "tot_num_correct = 0; tot_num_samples = 0\n",
    "test_i = 0\n",
    "conf_n_correct_list      = np.zeros(len(conf_thres_list))\n",
    "conf_tot_n_samples_list  = np.zeros(len(conf_thres_list))\n",
    "n_to_detect_list       = np.zeros(len(conf_thres_list))\n",
    "for i, name in enumerate(eval_dataset_mlp.idx_2_name):\n",
    "  if name == 'survey_desktop':\n",
    "    human_idx = i\n",
    "assert eval_dataset_mlp.idx_2_name[human_idx] == 'survey_desktop'\n",
    "blocksize_mlp = 70\n",
    "for batch in eval_dataloader:\n",
    "  _, time_diff, pos_x, pos_y, _,terminate_idx = batch\n",
    "  time_diff=time_diff.to(device);  \n",
    "  pos_x=pos_x.to(device);          pos_y=pos_y.to(device);\n",
    "  end_idx_raw = np.random.randint(10,max_sample_len_eval)\n",
    "  assert 10<= end_idx_raw<max_sample_len_eval\n",
    "\n",
    "  assert terminate_idx[0]==max_sample_len_eval\n",
    "  x2 = int(pos_x[0,0].cpu().item())\n",
    "  y2 = int(pos_y[0,0].cpu().item())\n",
    "  t2 = int(time_diff[0,0].cpu().item())  #todo   disc 2 token\n",
    "  start_xyt = (tc.tensor([x2,y2,t2], dtype=tc.long, device=device)[None, ...])\n",
    "  \n",
    "  ini_xyt = tc.zeros((1,max_new_tokens),dtype=tc.long, device=device)\n",
    "  end_idx = end_idx_raw*3\n",
    "  ini_xyt[0,0] = pos_x[0,0]   #x \n",
    "  ini_xyt[0,1] = pos_y[0,0]   #y\n",
    "  ini_xyt[0,2] = time_diff[0,0]    #t\n",
    "  ini_xyt[0,end_idx] = pos_x[0,end_idx_raw] #x  \n",
    "  ini_xyt[0,end_idx+1] = pos_y[0,end_idx_raw] #y\n",
    "  ini_xyt[0,end_idx+2] = time_diff[0,end_idx_raw]    #t\n",
    "  # gen_xyt = model.generate(start_xyt, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "  gen_xyt = generate(model,tc.clone(ini_xyt),end_idx, temperature=temperature, top_k=top_k)\n",
    "  bs,bl = gen_xyt.shape \n",
    "  gen_xyt = gen_xyt.reshape(bs,-1,3)   \n",
    "  pos_x_gen = gen_xyt[:,:,0].to(tc.float32)[:,:blocksize_mlp]\n",
    "  pos_y_gen = gen_xyt[:,:,1].to(tc.float32)[:,:blocksize_mlp]\n",
    "  time_diff_gen_token = gen_xyt[:,:,2]\n",
    "  time_diff_gen = disc.token_2_cont(time_diff_gen_token.cpu())[:,:blocksize_mlp].to(device)  #todo use log  or just use continous prediction\n",
    "  y_target = tc.tensor([human_idx]).to(device)\n",
    "  for conf_i,conf_thres in enumerate(conf_thres_list):\n",
    "    max_conf = 0;\n",
    "    n_to_detect = 0;   \n",
    "    pos_x2=tc.zeros_like(pos_x_gen);\n",
    "    pos_y2=tc.zeros_like(pos_y_gen);time_diff2=tc.zeros_like(time_diff_gen)  \n",
    "    while n_to_detect<max_sample_len_eval and max_conf<conf_thres:\n",
    "      #**  request more data if conf is lower then threshold\n",
    "      pos_x2[0,n_to_detect] = pos_x_gen[0,n_to_detect]\n",
    "      pos_y2[0,n_to_detect] = pos_y_gen[0,n_to_detect]\n",
    "      time_diff2[0,n_to_detect] = time_diff_gen[0,n_to_detect] \n",
    "      y_hat = model_mlp(time_diff2,pos_x2,pos_y2)\n",
    "      conf_y_hat = tc.softmax(y_hat,dim=1)\n",
    "      max_conf = conf_y_hat.max(dim=1)[0]\n",
    "      n_to_detect += 1   \n",
    "      # breakpoint()\n",
    "      # print(ii,\"max_conf\",max_conf)\n",
    "    _, predictions = y_hat.max(1)\n",
    "    n_correct = (predictions == y_target).sum();\n",
    "    n_sample  = predictions.size(0)  #*1\n",
    "    n_to_detect_list[conf_i] += n_to_detect\n",
    "    conf_n_correct_list[conf_i] += n_correct\n",
    "    conf_tot_n_samples_list[conf_i] += n_sample\n",
    "    tot_num_correct += n_correct\n",
    "    tot_num_samples += n_sample\n",
    "  test_i+=1\n",
    "  print(f\"{test_i}/{eval_len}___\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_n_to_detect_list = n_to_detect_list / eval_len\n",
    "assert tot_num_samples == (test_i * bs_eval *len(conf_thres_list)) == eval_len*len(conf_thres_list)\n",
    "print(f\"Accuracy on test set: {tot_num_correct/tot_num_samples*100:.2f}\")\n",
    "accu_list = conf_n_correct_list/conf_tot_n_samples_list\n",
    "suptitle = 'OffenseTask Unimodal'\n",
    "title = \"Accuracy vs Number of Events to Detection\"\n",
    "save_path = f\"./img/b_offenseTask_{training_len=}.png\"\n",
    "plot_accu(avg_n_to_detect_list, accu_list, title,save_path,\n",
    "          suptitile=suptitle,\n",
    "          xlabel='Number of Events to Detection',\n",
    "          ylabel='Probability of Correct Classification')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
